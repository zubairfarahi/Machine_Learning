# Exercise 1 - Linear Regression
- getting reaccustomed to *Octave*;
- trying to vectorise the code as much as possible.

## Univariate Linear Regression
- the data is first of all plotted in order to observe the distribution of the features;
- next, the features are normalised by subtracting their mean value and then dividing the result by
the standard deviation;
- then the cost function *J* is minimised for the new set of features via *Gradient Descent*.

## Multivariate Linear Regression
- the algorithm follows the lines of the former;
- now there's a heavier emphasis on vectorisation as the size of theta varies.
